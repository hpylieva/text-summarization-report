\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{baseline_NMT}
\citation{text_sum_survey}
\citation{abstractive_text_summarization}
\citation{attention_based_NMT}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!40}{\leavevmode {\color  {green!40}o}}\ Invent a meaningful abstract.}{1}}
\pgfsyspdfmark {pgfid1}{20112834}{45458471}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!40}{\leavevmode {\color  {green!40}o}}\ Invent a meaningful introduction. Per Shelpuk: Good project will be dedicated to an important problem and have a clear vision of what value it can bring to the potential users. All stages will be explored and analyzed, the approach for each of them is selected thoughtfully, compared to the alternatives and clearly explained. The results are evaluated and explained (explanation should provide additional information, not just restating the results or the code in English).}{1}}
\pgfsyspdfmark {pgfid2}{20112834}{35665971}
\citation{attention_based_NMT}
\citation{attention_based_NMT}
\citation{seq2seq_with_NN}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The model reads an input sentence \IeC {\textquotedblleft }ABC\IeC {\textquotedblright } and produces \IeC {\textquotedblleft }WXYZ\IeC {\textquotedblright } as the output sentence. The model stops making predictions after outputting the $<$eos$>$ token. It then starts emiting one target word at a time. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:seq-to-seq}{{1}{2}}
\citation{baseline_NMT}
\citation{LSTM_baseline}
\bibcite{baseline_NMT}{{1}{}{{}}{{}}}
\bibcite{text_sum_survey}{{2}{}{{}}{{}}}
\bibcite{abstractive_text_summarization}{{3}{}{{}}{{}}}
\bibcite{attention_based_NMT}{{4}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Recurrent Neural Network\relax }}{3}}
\newlabel{fig:RNN}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Overview}{3}}
\bibcite{seq2seq_with_NN}{{5}{}{{}}{{}}}
\bibcite{LSTM_baseline}{{6}{}{{}}{{}}}
\bibcite{basic-article}{{7}{}{{}}{{}}}
\bibcite{bishop}{{8}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
