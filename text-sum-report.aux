\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{text_sum_survey}
\citation{abstractive_text_summarization}
\citation{attention_based_NMT}
\citation{attention_based_NMT}
\citation{attention_based_NMT}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{1}{subsection.2.1}}
\citation{seq2seq_with_NN}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The model reads an input sentence \IeC {\textquotedblleft }A B C D\IeC {\textquotedblright } and produces \IeC {\textquotedblleft }X Y Z\IeC {\textquotedblright } one word by word as output sentence. The model stops making predictions after outputting the $<$eos$>$ token. \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:seq-to-seq}{{1}{2}{The model reads an input sentence “A B C D” and produces “X Y Z” one word by word as output sentence. The model stops making predictions after outputting the $<$eos$>$ token. \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Recurrent Neural Network\relax }}{2}{figure.caption.2}}
\newlabel{fig:RNN}{{2}{2}{Recurrent Neural Network\relax }{figure.caption.2}{}}
\citation{baseline_NMT}
\citation{LSTM_baseline}
\citation{seq2seq_with_NN}
\citation{model-baseline-article}
\citation{reference-repository}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}LSTM}{3}{subsubsection.2.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Overview}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Encoder}{3}{subsubsection.2.2.1}}
\citation{seq2seq-explanation}
\citation{seq2seq-explanation}
\citation{scheduled_sampling}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Decoder}{4}{subsubsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces LSTM-based sequence to sequence model on example of NMT. Note: $v$ is actually fed on input of decoder on each step to prevent neural network from "forgetting" about it till the end of output sequence. \relax }}{4}{figure.caption.3}}
\newlabel{fig:encoder-decoder-lstm}{{3}{4}{LSTM-based sequence to sequence model on example of NMT. Note: $v$ is actually fed on input of decoder on each step to prevent neural network from "forgetting" about it till the end of output sequence. \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Attention}{4}{subsubsection.2.2.3}}
\newlabel{attention}{{2.2.3}{4}{Attention}{subsubsection.2.2.3}{}}
\citation{attention_based_NMT}
\citation{attention_based_NMT}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Global Attention in an Encoder-Decoder Recurrent Neural Network\relax }}{5}{figure.caption.4}}
\newlabel{fig:attention}{{4}{5}{Global Attention in an Encoder-Decoder Recurrent Neural Network\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Training details}{5}{subsubsection.2.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dataset}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Preprocessing}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results Analysis}{7}{section.4}}
\bibcite{text_sum_survey}{{1}{}{{}}{{}}}
\bibcite{abstractive_text_summarization}{{2}{}{{}}{{}}}
\bibcite{attention_based_NMT}{{3}{}{{}}{{}}}
\bibcite{baseline_NMT}{{4}{}{{}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example predictions\relax }}{8}{table.caption.5}}
\newlabel{tab:results-table}{{1}{8}{Example predictions\relax }{table.caption.5}{}}
\bibcite{LSTM_baseline}{{5}{}{{}}{{}}}
\bibcite{seq2seq_with_NN}{{6}{}{{}}{{}}}
\bibcite{model-baseline-article}{{7}{}{{}}{{}}}
\bibcite{reference-repository}{{8}{}{{}}{{}}}
\bibcite{seq2seq-explanation}{{9}{}{{}}{{}}}
\bibcite{scheduled_sampling}{{10}{}{{}}{{}}}
\bibcite{bleu}{{11}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
