\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{baseline_NMT}
\citation{text_sum_survey}
\citation{abstractive_text_summarization}
\citation{attention_based_NMT}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!40}{\leavevmode {\color  {green!40}o}}\ Invent a meaningful abstract.}{1}}
\pgfsyspdfmark {pgfid1}{20112834}{45458471}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{green!40}{\leavevmode {\color  {green!40}o}}\ Invent a meaningful introduction. Per Shelpuk: Good project will be dedicated to an important problem and have a clear vision of what value it can bring to the potential users. All stages will be explored and analyzed, the approach for each of them is selected thoughtfully, compared to the alternatives and clearly explained. The results are evaluated and explained (explanation should provide additional information, not just restating the results or the code in English).}{1}}
\pgfsyspdfmark {pgfid2}{20112834}{35665971}
\citation{attention_based_NMT}
\citation{attention_based_NMT}
\citation{seq2seq_with_NN}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The model reads an input sentence \IeC {\textquotedblleft }ABC\IeC {\textquotedblright } and produces \IeC {\textquotedblleft }XYZ\IeC {\textquotedblright } as the output sentence. The model stops making predictions after outputting the $<$eos$>$ token. It then starts emiting one target word at a time. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:seq-to-seq}{{1}{2}}
\citation{baseline_NMT}
\citation{LSTM_baseline}
\citation{seq2seq_with_NN}
\citation{basic-article}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Recurrent Neural Network\relax }}{3}}
\newlabel{fig:RNN}{{2}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}LSTM}{3}}
\citation{scheduled_sampling}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Overview}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Encoder}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Decoder}{4}}
\@writefile{tdo}{\contentsline {todo}{\color@fb@x {}{black}{}{orange}{\leavevmode {\color  {orange}o}}\ Does this sentence easy to understand?}{4}}
\pgfsyspdfmark {pgfid3}{6938905}{28851677}
\pgfsyspdfmark {pgfid6}{37602919}{28866422}
\pgfsyspdfmark {pgfid7}{39028327}{28597725}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Attention}{4}}
\newlabel{attention}{{2.2.3}{4}}
\bibcite{baseline_NMT}{{1}{}{{}}{{}}}
\bibcite{text_sum_survey}{{2}{}{{}}{{}}}
\bibcite{abstractive_text_summarization}{{3}{}{{}}{{}}}
\bibcite{attention_based_NMT}{{4}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces LSTM-based sequence to sequence model on example of NMT. Note: $v$ is actually fed on input of decoder on each step to prevent neural network from "forgetting" about it till the end of output sequence. \relax }}{5}}
\newlabel{fig:encoder}{{3}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Training details}{5}}
\bibcite{seq2seq_with_NN}{{5}{}{{}}{{}}}
\bibcite{LSTM_baseline}{{6}{}{{}}{{}}}
\bibcite{basic-article}{{7}{}{{}}{{}}}
\bibcite{scheduled_sampling}{{8}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
